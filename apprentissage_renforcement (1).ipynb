{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cc407fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "434f7e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# nb max de clients est 130\n",
    "n_clients=50\n",
    "def data_prep(n_clients):\n",
    "    n_depot = n_clients+1\n",
    "    #on importe les donnée sur les clients et le depot\n",
    "    df=pd.read_excel(\"2_detail_table_customers.xls\")\n",
    "    df_depot=pd.read_excel('4_detail_table_depots.xls')\n",
    "    #on extrait les informations qui nous intéressent\n",
    "    Columns=[\"CUSTOMER_NUMBER\",\"CUSTOMER_LATITUDE\",\"CUSTOMER_LONGITUDE\"]\n",
    "    Data =( df[Columns]\n",
    "           .rename(columns={\"CUSTOMER_NUMBER\":\"number\",\"CUSTOMER_LATITUDE\":\"x\",\"CUSTOMER_LONGITUDE\":\"y\"})\n",
    "          )\n",
    "    Data=Data.head(n_clients)\n",
    "    depot = df_depot[['DEPOT_CODE','DEPOT_LATITUDE','DEPOT_LONGITUDE']].drop_duplicates()\n",
    "    depot = depot.rename(columns={\"DEPOT_CODE\":\"number of client/depot\",\"DEPOT_LATITUDE\":\"x\",\"DEPOT_LONGITUDE\":\"y\"})\n",
    "    Data.loc[len(Data)]=depot.iloc[0]\n",
    "    return Data\n",
    "def client_distances(Data):\n",
    "    #définition d'une matrice U qui contient les distances entre les clienst/depot\n",
    "    U=np.zeros((len(Data),len(Data)))\n",
    "    for i in range(len(Data)):\n",
    "        for j in range(i+1,len(Data)):\n",
    "            U[i,j]=np.sqrt((Data.iloc[i,1]-Data.iloc[j,1])**2+(Data.iloc[i,2]-Data.iloc[j,2])**2)\n",
    "            U[j,i]=U[i,j]\n",
    "    return U\n",
    "Data=data_prep(n_clients)\n",
    "U=client_distances(Data)\n",
    "\n",
    "\n",
    "def fonction_objectif( L,M=U):\n",
    "    if L==None:\n",
    "        return 100\n",
    "    time=0\n",
    "    V=30\n",
    "    for i in range(len(L)-1):\n",
    "        time+=U[L[i]-1,L[i+1]-1]\n",
    "    return time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50cfbd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(nb_states, state, epsilon):\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        # Exploration : choisir une action aléatoire\n",
    "        return np.random.randint(nb_states)\n",
    "    else:\n",
    "        # Exploitation : choisir l'action avec la valeur Q maximale\n",
    "        return np.argmax(Q[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e00848c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_states=8\n",
    "nb_actions=8\n",
    "epsilon=0.5\n",
    "alpha=0.1 #learning rate\n",
    "gamma=0.9 #discount rate\n",
    "num_episodes=20\n",
    "\n",
    "def Q_learning(num_episodes, alpha, gamma, epsilon, reward):\n",
    "    Q = np.zeros(nb_states)  # Initialiser la table Q\n",
    "    for episode in range(num_episodes):\n",
    "        state = np.randint(1,9)  # État initial\n",
    "        while not terminal_state(state):\n",
    "            action = epsilon_greedy_policy(nb_actions, state, epsilon)  # Choix de l'action\n",
    "            sol, next_state, next_action, reward = apply_action(state, action)  # Application de l'action\n",
    "            Q[state,action]=(1-alpha)*Q[state,action]+alpha*(reward+gamma*Q[next_state, next_action])\n",
    "            state=next_state\n",
    "    return Q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc61d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour passer d'un état à un autre avoir - la fonction objectif dans Q\n",
    "\n",
    "def apply_action(sol, action):\n",
    "    # Appliquer l'action et calculer la récompense\n",
    "    if action == 1:\n",
    "        sol = intra_route_swap(sol)\n",
    "    elif action == 2:\n",
    "        sol = inter_route_swap(sol)\n",
    "    elif action == 3:\n",
    "        sol = intra_route_shift(sol)\n",
    "    elif action == 4:\n",
    "        sol = inter_route_shift(sol)\n",
    "    elif action == 5:\n",
    "        sol = two_intra_route_swap(sol)\n",
    "    elif action == 6:\n",
    "        sol = two_intra_route_shift(sol)\n",
    "    elif action == 7:\n",
    "        sol = eliminate_smallest_route(sol)\n",
    "    elif action == 8:\n",
    "        sol = eliminate_random_route(sol)\n",
    "    \n",
    "    next_state=action\n",
    "    next_action = epsilon_greedy_policy(nb_states, next_state, epsilon)\n",
    "    # Calculer la récompense inversée\n",
    "    reward_inverse = fonction_objectif(sol)\n",
    "    \n",
    "    # Retourner la nouvelle solution et la récompense inversée\n",
    "    return sol, next_state, next_action, -reward_inverse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aed9b41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonctions pour les 8 actions\n",
    "#action changer d'une fonction de voisinage à un autre\n",
    "nb_véhicules=2\n",
    "\n",
    "# 1. Intra-Route Swap: échange d'un client avec un autre client dans la même route\n",
    "def intra_route_swap(sol):\n",
    "    # Sélectionner une route au hasard\n",
    "    k=np.random.randint(1,nb_véhicules)\n",
    "    #Sélectionner 2 clients au hasard\n",
    "    i, j = random.sample(sol[k], 2)\n",
    "    # Échanger les positions des clients\n",
    "    sol[k][i], sol[k][j] = sol[k][j], sol[k][i]\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e27daa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Inter-Route Swap: déplacement d'échange d'un client d’une route avec un client d’une autre route\n",
    "def inter_route_swap(sol):\n",
    "    k1, k2 = random.sample(sol, 2)\n",
    "    # Sélectionner un client de chaque route au hasard\n",
    "    i1=np.random.randint(1, len(sol[k1]) - 1)\n",
    "    i2=np.random.randint(1, len(sol[k2]) - 1)\n",
    "    # Échanger les clients entre les routes\n",
    "    sol[k1][i1], sol[k2][i2] = sol[k2][i2], sol[k1][i1]\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "574611fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Intra-Route Shift: déplacement d'un client vers une autre position sur la même route\n",
    "def intra_route_shift(sol):\n",
    "    # Sélectionner une route au hasard\n",
    "    k=np.random.randint(1,nb_véhicules)\n",
    "    i1=np.random.randint(1,len(sol[k1])-1)\n",
    "    # Déplacer le client vers une autre position dans la route\n",
    "    client = sol[k].pop(i1)\n",
    "    j = random.randint(1, len(sol[k1])-1)  # Sélectionner une nouvelle position au hasard\n",
    "    sol.insert(j, client)\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8a75de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Inter-Route Shift: déplacement d'un client d’une route à une autre\n",
    "def inter_route_shift(sol):\n",
    "    k1, k2 = random.sample(sol, 2)\n",
    "    # Sélectionner un client de la première route et l'insérer dans la deuxième route\n",
    "    i1=np.random.randint(1,len(sol[k1])-1)\n",
    "    i2=np.random.randint(1,len(sol[k2])-1)\n",
    "    client = sol[k1].pop(i1)\n",
    "    sol[k2].insert(i2, client)\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "538d2972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Two Intra-Route Swap: échange de clients sur la même route (deux clients consécutifs sont échangés avec deux autres clients consécutifs de la même route)\n",
    "def two_intra_route_swap(sol):\n",
    "    k=np.random.randint(1,nb_véhicules)\n",
    "    # Sélectionner deux paires de clients consécutifs au hasard dans la route\n",
    "    i, j, u, l = random.sample(sol[k], 4)\n",
    "    # Échanger les paires de clients consécutifs\n",
    "    sol[k][i], sol[k][j], sol[k][u], sol[k][l] = sol[k][u], sol[k][l], sol[k][i], sol[k][j]\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7edb06b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Two Intra-Route Shift: relocalisation des clients sur la même route (deux clients consécutifs sont retirés de leur position et réinsérés dans une autre position de la même route)\n",
    "def two_intra_route_shift(sol):\n",
    "    k=np.random.randint(1,nb_véhicules)\n",
    "    # Sélectionner deux clients consécutifs au hasard dans la route\n",
    "    i = random.randint(1, len(sol[k])-2)\n",
    "    # Retirer les deux clients de leur position\n",
    "    clients = sol[k].pop(i), sol[k].pop(i)\n",
    "    # Insérer les clients dans une nouvelle position\n",
    "    j1, j2 = random.sample(sol[k], 2)\n",
    "    sol[k].insert(j1, clients[0])\n",
    "    sol[k].insert(j2, clients[1])\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07b7bdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Élimine la plus petite route: élimine la plus petite route de la solution\n",
    "def eliminate_smallest_route(sol):\n",
    "    # Trouver la plus petite route dans la solution\n",
    "    smallest_route = min(sol, key=len)\n",
    "    # Supprimer la plus petite route de la solution\n",
    "    sol.remove(smallest_route)\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57fe3e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Élimine une route aléatoire: élimine une route aléatoire de la solution\n",
    "def eliminate_random_route(sol):\n",
    "    # Sélectionner une route au hasard dans la solution\n",
    "    route_to_remove = random.choice(sol)\n",
    "    # Supprimer la route de la solution\n",
    "    sol.remove(route_to_remove)\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3afb5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChooseAnAction(state, type_function):\n",
    "    next_state = None\n",
    "    if type_function == 1:\n",
    "        next_state = epsilon_greedy_policy(nb_state, state, epsilon)\n",
    "    elif type_function == 2:\n",
    "        next_state = np.random.randint(nb_states)\n",
    "    \n",
    "    return next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8616b45",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 13 (2558729688.py, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 16\u001b[1;36m\u001b[0m\n\u001b[1;33m    state = next_state  # Mise à jour de l'état\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'for' statement on line 13\n"
     ]
    }
   ],
   "source": [
    "def adaptive_local_search_q_learning(x0):\n",
    "    Q = np.zeros((nb_states, nb_actions))  # Initialiser la table Q\n",
    "    for episode in range(num_episodes):\n",
    "        state = 0  # État initial\n",
    "        improved=True\n",
    "        no_improvement=0\n",
    "        x1=x0\n",
    "        x=x0\n",
    "        for episode in range(num_episodes):\n",
    "            reward=0\n",
    "            states_visited_count=0\n",
    "            next_state=ChooseAnAction(0,2)\n",
    "            x=bestNeighbor(next_state,x)\n",
    "            \n",
    "        while not terminal_state(state):\n",
    "            action = epsilon_greedy_policy(Q, state, epsilon)  # Choix de l'action\n",
    "            next_state, reward = apply_action(state, action)  # Application de l'action\n",
    "            best_next_action = np.argmax(Q[next_state])  # Meilleure action dans le prochain état\n",
    "            td_target = reward + discount_factor * Q[next_state, best_next_action]  # Cible TD\n",
    "            td_error = td_target - Q[state, action]  # Erreur TD\n",
    "            Q[state, action] += learning_rate * td_error  # Mise à jour de Q\n",
    "            # Exploration locale\n",
    "            for _ in range(10):  # Effectuer 10 itérations de recherche locale (modifiable)\n",
    "                # Effectuer une recherche locale ici (par exemple, permutation d'actions, etc.)\n",
    "                # Mettre à jour Q en fonction des résultats de la recherche locale\n",
    "            state = next_state  # Mise à jour de l'état\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9563ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestNeighbor(next_state,x):\n",
    "    for i in range()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdd91c1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "random_sample() takes at most 1 positional argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mmtrand.pyx:4855\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.sample\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mmtrand.pyx:384\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.random_sample\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: random_sample() takes at most 1 positional argument (2 given)"
     ]
    }
   ],
   "source": [
    "            best_next_action = np.argmax(Q[next_state])  # Meilleure action dans le prochain état\n",
    "            td_target = reward + discount_factor * Q[next_state, best_next_action]  # Cible TD\n",
    "            td_error = td_target - Q[state, action]  # Erreur TD\n",
    "            Q[state, action] += learning_rate * td_error  # Mise à jour de Q\n",
    "            state = next_state  # Mise à jour de l'état"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbc4fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
